{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TalkingData AdTracking Fraud Detection Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who Am I\n",
    "\n",
    "- My name is Matt [@lstmemery](https://github.com/lstmemery)\n",
    "- I'm a data scientist at Imbellus\n",
    "- My company builds simulations to test problem solving ability\n",
    "- I use the telemetry from the simulations to build models for prediction\n",
    "- I first got interested in data science through this group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is TalkingData?\n",
    "- TalkingData, Chinaâ€™s largest independent big data service platform\n",
    "- 3 Billion clicks per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Competition\n",
    "\n",
    "- Find the fradulent click based on IP, App, Device etc.\n",
    "![Click Farmer](./img/click_farmer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data\n",
    "- 5 categorical columns (IP, App, Device (iPhone 6+), OS, Channel (Ad Publisher))\n",
    "- 1 datetime column (click_time)\n",
    "- 0.25 **Billion** Rows representing 3 days of logs\n",
    "- Predict a binary \"is_attributed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53918</th>\n",
       "      <td>97670</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>121</td>\n",
       "      <td>2017-11-08 03:34:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19802</th>\n",
       "      <td>48240</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-07 08:39:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69472</th>\n",
       "      <td>48219</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>116</td>\n",
       "      <td>2017-11-08 07:33:55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ip  app  device  os  channel           click_time  is_attributed\n",
       "53918  97670   18       1  19      121  2017-11-08 03:34:42              0\n",
       "19802  48240   64       1  19      459  2017-11-07 08:39:20              0\n",
       "69472  48219   22       1  13      116  2017-11-08 07:33:55              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train_sample.csv\")\\\n",
    "    .drop(columns=[\"attributed_time\"])\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train vs Test\n",
    "\n",
    "Training data: November 6th to 9th\n",
    "Test data: Some of the 10th\n",
    "\n",
    "![https://www.kaggle.com/gopisaran/indepth-eda-entire-talkingdata-dataset](./img/train_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unique Values\n",
    "\n",
    "![https://www.kaggle.com/kailex/talkingdata-eda-and-class-imbalance](./img/feature_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This is a VERY imbalanced dataset\n",
    "\n",
    "\n",
    "![https://www.kaggle.com/gopisaran/indepth-eda-entire-talkingdata-dataset](./img/class_imbalance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scoring\n",
    "- ROC-AUC\n",
    "- Top Score: 0.9843223\n",
    "- 2nd Best: 0.9841256\n",
    "![https://www.kaggle.com/pranav84/talkingdata-eda-to-model-evaluation-lb-0-9683](./img/example_roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Is BIG DATA?\n",
    "\n",
    "- If it's a pain in the ass to do basic operations like counting in RAM, it's big data\n",
    "- This is dependent on your computer and libraries you use\n",
    "> pandas rule of thumb: have 5 to 10 times as much RAM as the size of your dataset -- Wes McKinney, creator of Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You Can Always Rent a Bigger Instance!\n",
    "\n",
    "![https://www.slideshare.net/AmazonWebServices/aws-reinvent-2016-optimizing-workloads-in-sap-hana-with-amazon-ec2-x1-instances-cmp322](./img/aws_x1.jpg)\n",
    "\n",
    "But it'll cost you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dask (and Parallel computing)\n",
    "\n",
    "- Dask offers multithreaded, multiprocessing and cluster computing options for numpy, pandas and scikit-learn\n",
    "- Basic idea: take a really big array, chop it into managable pieces and add it to a collection\n",
    "- The underlying data structure is a bag (think a set with repeats)\n",
    "\n",
    "![https://dask.pydata.org/en/latest/](./img/dask_task_graph.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ip\n",
       "113155    1\n",
       "13920     1\n",
       "18507     1\n",
       "71491     1\n",
       "16453     6\n",
       "14903     3\n",
       "199848    1\n",
       "162711    1\n",
       "29086     2\n",
       "195934    1\n",
       "Name: app, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install \"dask[dataframe]\"\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\"data/train_sample.csv\")\n",
    "print(type(df))\n",
    "apps_by_ip = df.groupby(\"ip\")[\"app\"].nunique().compute()\n",
    "print(type(apps_by_ip))\n",
    "apps_by_ip.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use BigQuery\n",
    "\n",
    "- BigQuery is a big data service that acts like SQL\n",
    "- This is Google service. You pay for storage (0.023 USD per GB, per month) and queries (first Terabyte each month is free)\n",
    "- There's a real nice pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app_nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>183199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>25118</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>124047</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>194308</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>41232</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ip  app_nunique\n",
       "382  183199            1\n",
       "746   25118            3\n",
       "421  124047            3\n",
       "700  194308            4\n",
       "255   41232           10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install pandas-gbq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  ip,\n",
    "  COUNT(DISTINCT(app)) AS app_nunique\n",
    "FROM\n",
    "  [tactile-bindery-675:talkingdata.train_sample]\n",
    "GROUP BY\n",
    "  ip\n",
    "LIMIT\n",
    "  1000\n",
    "\"\"\"\n",
    "ip_app_count = pd.read_gbq(query, project_id=\"tactile-bindery-675\")\n",
    "ip_app_count.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Undersample\n",
    "\n",
    "- So Much Data + Data Imbalance = A Good Candidate for Undersampling\n",
    "- Undersampling means removing examples of the majority class until some point\n",
    "- Different under/oversampling algorithms can be found in the [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(454, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/train_sample.csv\", \n",
    "                 parse_dates=[\"click_time\"])\n",
    "print(df.shape)\n",
    "df[\"click_time\"] = df[\"click_time\"].astype(np.int64) #turn the datetime to a unix timestamp\n",
    "features = df[[\"ip\", \"device\", \"os\", \"channel\", \"click_time\"]]\n",
    "target = df[\"is_attributed\"]\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=0)\n",
    "undersampled_features, undersampled_target = undersampler.fit_sample(features, target)\n",
    "undersampled_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Note about Data Leakage\n",
    "\n",
    "- Many competitors (including the winners) leaked distribution information to their models\n",
    "- Data leakage is when the model is exposed to validation/test set information\n",
    "- Data leakge results in overconfident models\n",
    "- A really common example in this case is pre-computing grouped features in BigQuery using train and test set\n",
    "- It's possible to do this in a Kaggle competition but would be impossible to do in production\n",
    "- **RULE OF THUMB: If you are doing anything that couldn't be done one row at a time, it's a potential source of data leakage**\n",
    "![https://www.kaggle.com/dansbecker/data-leakage](./img/data_leakage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation Strategies\n",
    "\n",
    "- Validation is when you leave some training data in reserve to test locally\n",
    "- The computation cost of running an algorithm over the whole dataset means that validation was critical\n",
    "- Generally, there were two approaches:\n",
    "\n",
    "    1. Ignore the time series characteristic of the data and use cross validation\n",
    "    2. Set aside certain periods of time of the last day of the data\n",
    "\n",
    "Evaluate your validation strategy by seeing how well your validation score predicts your test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Prize (0.9843223)\n",
    "\n",
    "- Undersampled to equality, throwing out 99.8% of the data\n",
    "- Bagged five different predictors with five different undersampled datasets\n",
    "- Did all of the basic feature engineering summary statistics\n",
    "- Final model was 7 bagged LightGBM models and a bagged neural net\n",
    "- Validated on the final day of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical Feature Embedding\n",
    "\n",
    "- Take any two categorical features (say ip and app id)\n",
    "- Find all the app ids for a given ip and concatenate them to together as a sentence\n",
    "- Run your favorite topic model over the feature \"sentence\"\n",
    "- Team used non-negative matrix factorization, latent Dirichlet allocation and latent semantic analysis\n",
    "- Limit 5 topics per embedding\n",
    "\n",
    "$5 * 4 \\textrm{ categorical combinations} * 5 \\textrm{ topics per embedding} * 3 \\textrm{ types of embedding} = 300 \\textrm{ new features}$\n",
    "\n",
    "Score change: 0.9821 to 0.9828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "apps_of_ip = {}\n",
    "for sample in data_samples:\n",
    "  apps_of_ip.setdefault(sample['ip'], []).append(str(sample['app']))\n",
    "ips = list(apps_of_ip.keys())\n",
    "apps_as_sentence = [' '.join(apps_of_ip[ip]) for ip in ips]\n",
    "apps_as_matrix = CountTokenizer().fit_transform(apps_as_sentence)\n",
    "topics_of_ips = LDA(n_components=5).fit_transform(apps_as_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Second Prize\n",
    "\n",
    "- Subsampled for training, final model was trained on all data\n",
    "- Created 100s of features but none of them were particularly interesting\n",
    "- Ensembled a ton of LightGBM models using weights inferred from the public leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Third Prize\n",
    "\n",
    "- The only one of the top that used RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(self):    \n",
    "    categorial_inp = Input(shape=(len(self.categorical),))\n",
    "    cat_embeds = []\n",
    "    for idx, col in enumerate(self.categorical):\n",
    "        x = Lambda(lambda x: x[:, idx,None])(categorial_inp)\n",
    "        x = Embedding(self.categorical_num[col][0], self.categorical_num[col][1],input_length=1)(x)\n",
    "        cat_embeds.append(x)\n",
    "    embeds = concatenate(cat_embeds, axis=2)\n",
    "    embeds = GaussianDropout(0.2)(embeds)\n",
    "    continous_inp = Input(shape=(len(self.continous),))\n",
    "    cx = Reshape([1,len(self.continous)])(continous_inp)\n",
    "    x = concatenate([embeds, cx], axis=2)\n",
    "    x = CuDNNGRU(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.05)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fourth Prize\n",
    "\n",
    "- Used Weight of Evidence Encoding for high cardinality categories\n",
    "\n",
    "$$WoE = \\ln{\\frac{\\% non-events}{\\% events}}$$\n",
    "\n",
    "So if IP $x$ had 4 non-events and 2 events:\n",
    "\n",
    "$$WoE_x = \\ln{\\frac{\\frac{4}{6}}{\\frac{2}{6}}} = \\ln{\\frac{0.667}{0.333}} = \\ln{2} = 0.693$$\n",
    "\n",
    "They said this didn't work well, but it may be worth exploring in other competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sixth Prize\n",
    "- Used a Keras implementation of libFM\n",
    "- libFM is a matrix factorization library that generalizes to more than two factors\n",
    "https://github.com/jfpuget/LibFM_in_Keras/blob/master/keras_blog.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
